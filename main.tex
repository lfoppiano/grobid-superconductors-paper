\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{authblk}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amssymb}
\usepackage{tabularray}
\usepackage{tabularx}

\newcommand{\tc}{T$_{c}$}
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}


\title{Automatic Extraction of Materials and Properties from Superconductors Scientific Literature}
\author[1]{Luca Foppiano}
\author[2]{Pedro Baptista de Castro}
\author[3]{Pedro Ortiz Suarez}
\author[2]{Kensei Terashima}
\author[2]{Yoshihiko Takano}
\author[1]{Masashi Ishii}

\affil[1]{Material Database Group, MaDIS, NIMS, Tsukuba, Japan}
\affil[2]{Nano Frontier Superconducting Materials Group, MANA, NIMS, Tsukuba, Japan}
\affil[3]{Data and Web Science Group, University of Mannheim, Mannheim, Germany}

\begin{document}

\maketitle

\begin{abstract}
The automatic extraction of materials and related properties from the scientific literature is getting more attention in data-driven materials science (Materials Informatics). 
We present our system for automatically extract superconductor material names and respective properties from PDF documents.
Our system combines Machine Learning and Heuristic approaches using a flexible architecture: developers can plug-in various Named Entities Recognition (NER) methods (CRF, BidLSTM, Transformers) or relation extraction approaches (ruled-based, CRF).
We have processed 37700 papers and extracted 40324 materials-properties records. 
The material, or sample information is represented by name, chemical formula, and material class, and characterised by shape, doping, variables for components, substrate as adjoined information.
The properties include the T\textsubscript{c} superconducting critical temperature and, when available, applied pressure with T\textsubscript{c} measurement method.
\end{abstract}

\keywords{material informatics, superconductors, machine learning, nlp, tdm}

\section{Introduction}
% 1. Start about SC, introduce few applications (Stanev), most of SC of discovery by "feeling" or "serendipidity"
%The research of new superconducting materials is still an hot topic in both fundamental science and practical applications.
%Superconductors display many interesting quantum phenomena including persistent electrical currents, zero-resistivity, ability to host a strong magnetic field, vortex pinning, and quantisation of the magnetic flux. [add references]
%Moreover, there is a growing list of practical and theoretical applications of superconductors including medical instruments (MRI/MNR), high-speed trains, quantum computers, and components of particles accelerators, such as the Linear Hadron Collider (LHC)~\cite{PhilippeBook, Kizu2010ConstructionOT, Cardani2017NewAO}.
%However, discovering a new superconductor is still a challenging task~\cite{PhysRevB.103.014509, doi:10.1088/1468-6996/16/3/033503} because the phenomena is not yet fully understood and the relation between superconductivity and chemical composition remains unclear. 
%Most of the superconductors materials have been discovered using the "feeling" (or "serendipity") approach following researchers intuition rather than any particular methodology.~\cite{doi:10.1080/08957959.2019.1695253}

% 2. Use of data-driven approaches is becoming more popular and fundamental for material discovery (there are many reviews - example citations data from Pedro) 
% ~\cite{stanev_machine_2017, le2020critical,Hamlin2019SuperconductivityNR}.(Superdocutivity releated citations)

% Pedro
In recent years, with the creation of computational databases, such as the Materials Project (MP)~\cite{materialsprojectJain2013}, the Open Quantum Materials Database (OQMD)~\cite{oqmdkirklin2015open}, and then experimental data repositories such as NIMS MDR (\url{http://mdr.nims.go.jp})~\cite{ranganathan_anusha_2019_3553963}, the focus has been steadily shifting towards a data-driven design of materials which is often called as Materials Informatics (MI). 
Such an approach is expected to accelerate exploration of functional materials, as it does not rely on intuition of very little genius researchers nor on their limited experience.
In this new paradigm, the efficient use of data to guide experiments and materials property prediction through use of machine learning methods takes the centre stage. 
For example, data-driven methods have been used to search/design magneto-caloric materials~\cite{Bocarsly2017,Castro2020,court2021inverse}, photo-catalysts for hydrogen splitting~\cite{xiong2021optimizing}, thermoeletrics~\cite{iwasaki2019machine}, and superconductors~\cite{stanev_machine_2017}. 
In such a data-driven search, one of the most important keys lies at the availability of the data, that at least should consist of compositions of materials and their physical properties. 
In the specific case of superconductivity, most of the data-driven works~\cite{stanev_machine_2017, le2020critical,Hamlin2019SuperconductivityNR} relies on a single database: SuperCon (\url{http://supercon.nims.go.jp}). 
SuperCon is a structured database of superconductors materials and their properties, developed at the National Institute for Materials Science (NIMS) in Japan. 

%Furthermore, the advance of natural language processing methods for data extraction of scientific literature, also plays a key role in supporting this new paradigm. 
%For example, \cite{court2018auto} developed a semi-automatic system to build a dataset of magnetic materials with their NÃ©el and Curie temperatures.  
%Other work from the same authors attempted to build a model for predicting properties, e.g. magnetic and superconducting critical temperature~\cite{court_magnetic_2020}. In the specific case of superconductivity, most of data-driven works relies on a single database: SuperCon\cite{stanev_machine_2017, le2020critical,Hamlin2019SuperconductivityNR}.

At the time of writing this paper, SuperCon contains about 33000 inorganic and 600 organic materials and is the ``de-facto'' standard in data-driven research for superconductors materials  (about 4400 articles contains the mention ``\textit{supercon database}'' in Google Scholar). 
However, SuperCon harvesting process is currently fully manual and the efficiency is directly proportional to the number of available human curators.
Curation is performed ``from scratch'': the human have to read the human-readable printed matter such as PDF document and manually enter the information in the system.
Considering the cost of database construction, it is necessary to consider an assisted or alternative system that improves throughput while ensuring data quality equivalent to that of manual extraction.

% 5. What are the needs? Need for a tool of methods allowing automatic extraction (both approach: general for materials or SuperCon)
% In summary, SuperCon needs to be modernised to support more data structures and to cope with the growing publications. 
%Ideally, our goal is to have a single tool that can be customised with ease to work in several domains. Nevertheless, at this stage, we focus only on superconductors materials research. 

As a solution, we are developing an hybrid data extraction methods from scientific literature combining automation using text data mining and manual curation.
The automated system pre-extracts and formats potential data and proposes them to the curator as ``pre-cooked'' structured data: (a) Highlight the relevant entities on the original document. 
(b) Pre-fill the extracted information in an editable tabular content. 
By linking (a) highlighted source text and (b) structured information, curators can quickly move back and forth between (a) and (b). 
In building the automatic part of this hybrid system, we used SuperMat~\cite{foppiano2021supermat}, which we recently constructed. 
This is a dataset linked to data annotated against the full text of the scientific literature on superconductivity research. 
We used SuperMat not only as training data for machine learning, but also for NLP evaluation.

% 6. in this work we present X, where we did Y and obtain Z
In this work, we present \textit{grobid-superconductors}: a system to extract automatically structured information of superconductors materials and properties from scientific literature. 
The tool is a specialised module of Grobid~\cite{GROBID}, a machine learning library to parse and structure scientific documents. 
Grobid provides an open source platform for building specialised modules: from astronomical entities recognition to software mentions and physical measurements extraction.
Grobid provide several features, including seamless integration with PDF documents native support, citation extraction and linking, bibliographic data consolidation via CrossRef-like service,  and a diverse set of ML architectures (for example Conditional Random Field (CRF), Recurrent Neural Networks (RNN) based on bidirectional LSTM, and transformers such as BERT~\cite{devlin2018bert} or SciBERT~\cite{Beltagy2019SciBERT}).

% 7. what is our purpose? 
Using grobid-superconductors and other sub-tools, we established a workflow for processing large quantity of documents and obtaining an automated database of superconductors materials and properties. 
We processed 37770 papers from ArXiv\footnote{\url{https://arxiv.org}} and obtained a database of 40324 records. 
This new database was named SuperCon2 as an automated staging system that would work with SuperCon through a curated interface. 
The project was also an opportunity to enrich SuperCon's database structure with new properties that reflect recent scientific trends.
Firstly, the ``pressure'' applied to obtain superconductivity, has gained attention because it can change radically the electronic structure of a material. 
Secondly, the ``method'' used to measure the superconducting transition temperature T\textsubscript{c} can be used to semantically recognise multiple T\textsubscript{c}s obtained from the same material or sample (e.g. distinguish calculated and experimental T\textsubscript{c}). 


\section{Grobid-superconductors}

% Overview from the other paper, what are the differences and some repetition on the previous paper 
\textit{Grobid-superconductor} is an extraction system that allow the processing of text or PDF documents to extract materials and corresponding properties. 
We develop \textit{grobid-superconductors} as a multi-steps tool based on the \textit{Grobid} library~\cite{GROBID} following some principles discussed in a previous preliminary study~\cite{foppiano:hal-02870896}.  
The choice to develop it as a \textit{Grobid} module bring several advantages: a) It integrates with \textit{pdfalto}\footnote{\url{https://github.com/kermitt2/pdfalto}}, a specialised tool for PDF parsing which mitigate extraction issues such as the resolution of embedded fonts, invalid character encoding, and the reconstruction of the correct reading order. b) It allows to access to PDF document layout information for both machine learning and document decoration and, finally, c) it provides access to a set of high-quality, pre-trained machine learning models for structuring documents.

% Abstract vs fulltext
\emph{Abstract vs fulltext}
The scope of \textit{grobid-superconductors} is to process the full-text of scientific articles except for figures and tables body requiring a specific process. 
At the time of writing this paper, we are aware of several related work applying machine learning on material data science that are developed with limitation of  only abstract text.
The main reason is that abstracts are usually freely available, are easier to obtain~\cite{kononova_text-mined_2019}, and contain condensed information~\cite{yamaguchi-etal-2020-sc, court_magnetic_2020}. 
Although accurately parsing the full text presents more challenges, it is possible to obtain a broader range of information, including detailed processes relevant to the material under study (e.g. behaviour with under or over-doping) related to the studied materials. Even more important full-text might contains negative results (e.g. absence of superconductivity for certain samples) which are rarely discussed in the abstract. 
Such examples are needed to supply knowledge of non-superconductivity when building a superconductors prediction model~\cite{stanev_machine_2017}. 
\textit{Grobid-superconductors} provides a Web REST API which can be used to set-up a large scale processing unit integrated with a decoupled ingestion workflow (Section~\ref{subsec:ingestion-workflow}).

% Sentences vs Paragraphs 
\emph{Paragraphs vs Sentences}
In grobid-superconductors we use sentences as the unit of text for training, evaluation and tagging.  
Intuitively, sentences are smaller, shorter and self-contained, and paragraphs can be very large (the abstract is usually a single paragraph). 
Moreover, for training deep learning models or transformers, shorter training examples uses less memory and allow training with larger batch size. 
Transformers are limited to 512 tokens (everything beyond that is ignored), depending on the tokenisation such limit can be easily passed. 
However, while sentences used for training are generally correct because they were manually verified, the sentences obtained automatically when processing real world documents might contain splitting mistakes, which could impact the final result. 
In general, the use of sentences or paragraphs based approach is very dependent on the task at hand. 
For this reason, we compared the results for the NER task for the two approaches using a sample of four documents (three for training and one for evaluation). We trained a deep learning model (BidLSTM\_CRF\_FEATURES, batch\_size:5) using both paragraphs and sentences. 
We evaluated the paragraph-based and obtained precision, recall and F1 to be 44.44\%, 27.21\%, and 33.76\%, respectively. Sentences-based evaluation resulted in 48.41\%, 50.00\%, and 51.70\% for precision, recall and F1, respectively. 
All indicators increased with the F1 that improved by 17.94\%.
For the Linking, in our previous preliminary work~\cite{foppiano2019proposal}, we noticed that paragraph-based linking could obtained a higher F1 of 18.01\% vs 11.87\% of sentence-based. 
This was explained by the boost in the recall which almost doubled (6.5\% to 10.7\%) but the precision decreased (68.7\% to 57\%). 
Using the sentence-based approach for Linking will inevitably result in loss of recall, however we will get more precise because it will also reduce the errors and the complexity of linking distant entities. 


% what makes us different from previous work 
% Grobid-superconductors supports PDFs documents natively. This means that a) the system as access to additional layout information such as superscript, subscript, bold, italic, font name, font size which can be exploited for improved accuracy. And, b) we collect the coordinates in the document of any information that is extracted from PDF document.
% This system was designed by improving a previous architecture discussed in ~\cite{foppiano2019proposal} where two main steps were designed for entity extraction and linking, respectively. 
% In addition this tool supports different ML architectures, including CRF, Recurrent Neural Networks (RNN) based on LSTM, and transformers where the contextual embedding can be selected between BERT and SciBERT. 
% We released it with an open source licence and the source code is publicly available on Github. In this way the tool development could follow the community needs as well as benefit from external contributors.  

% Contrary to all the related work mentioned before, we work with fulltext instead of abstract, which give use access to a) additional information and materials that might not be mentioned in the abstract, and b) mention to cases where the experiments recorded absence of superconductivity. 

% Our obtained database differs from SuperCon because it contains several additional properties, such as applied pressure, measurement method, crystal structure, and space groups. 
% The database also provide access to the "enhanced PDFs" where all the entities are highlighted with different colours for each type on top of the original PDF, exploiting the coordinates discussed before. 
% We believe, this features, can improve both the quality of the curations and their effectiveness. In annotation tasks (e.g. NER) it was demonstrated that visual pre-annotations were improving the task over several aspects, including time consumption and error rate~\cite{Fort2010InfluenceOP, Nvol2011SemiautomaticSA, Lingren2014EvaluatingTI}.

\subsection{Architecture}

\textit{Grobid-superconductors} is structured as a three-steps process: (a) ``Document structuring and pre-processing'' transforms the input document or text into the internal representation. (b) ``Extraction'' is a Named Entities Recognition (NER) where the text is processed to recognise and extract material and properties. Such extracted entities are combined. (c) ``Linking''  step which establishes relations between them. 

% \begin{figure}[ht]
% \includegraphics[width=\textwidth]{overview-schema}
% \caption{Overview of the architecture}
% \end{figure}

\subsubsection{Document structuring and pre-processing}
\label{subsubsec:document-structuring}
In the document pre-processing and structuring, the PDF document is converted into an internal model based on a list of text statements for subsequent internal processing.

% Internal process
The input document is processed using Grobid native models as illustrated in Figure~\ref{fig:grobid-document-processing}. After being transformed in XML (Extensible Markup Language) by \textit{pdfalto}, it is passed to the ``segmentation model'' which recognises the macro areas (header, body and annex)  As a result, the obtained sections are passed to specialised models in a cascade architecture. The header is further processed by the Header model, which further extracts the structure. We selectively extract title, abstract, keywords, publisher, publication year, DOI and ignore other items, such as authors, publication date. 
Body and annexes are processed by the ``Fulltext'' model that recognises paragraphs, reference markers (decorations (also called \textit{reference callout}) in the text referring to the citation references at the end of the paper), tables and figure zones.
Tables and figures zones are further decomposed by the respective models and we retain only the caption text. 

\begin{figure}[ht]
\label{fig:grobid-document-processing}
\includegraphics[width=\textwidth]{grobid-extraction-1}
\caption{Cascade architecture in the document structuring with Grobid.}
\end{figure}

The text is finally passed to a sentence segmenter (we use Apache OpenNLP (\url{https://opennlp.apache.org})) which splits paragraphs into sentences. We use the reference markers collected from the text improve the sentence segmentation process: the segmentation is cancelled if the end of sentence falls within the boundaries of one reference marker. For example is often the case that a reference in the form of \textit{Foppiano et. al.} is mistakenly recognised as the end of the sentence. This method is very effective at enhancing sentence segmentation in scientific text. 
The sentences are accumulated into special objects that represent general text passages containing the following properties: 
\begin{itemize}
    \item \textit{LayoutTokens} represent the token with layout information, such as style (italic, bold, superscript, and subscript), font (font type, font size), positions (coordinates (a list of pairs x,y), index position within the text chain), 
    \item \textit{section} contains the main sections: header, body and annex as they are divided from the Grobid ``document segmentation model'', 
    \item \textit{subsection} contains further division within the same section: paragraph, table or figure caption, abstract, title, 
    \item \textit{spans} are allocated to carry the extracted entities in the following step. One \textit{span} represents one entity with its type and position within the text. Spans can also store additional attributes using a key-value approach.
\end{itemize}

During the \textit{Document structuring and pre-processing}, the process extracts also the bibliographic information of the paper by combining the extracted information with a matching service to consolidate them against CrossRef\footnote{\url{https://www.crossref.org}}. In this way we ensure that the bibliographic data can match the publisher's information even if the paper was obtained from different sources, such as ArXiv. We collect the following bibliographic: title, authors, DOI, publisher, journal, and year of publication.

The following step of Extraction and Linking are enriching the internal structure by attaching additional structured information. 

\subsubsection{Extraction}
\label{subsubsec:extraction}

The second step in the architecture is the \textit{Extraction}, a named entity recognition (NER) component.
We designed it as aggregation of multiple ``parsers'' applied in sequence to the data structure with a final post-processing which perform cleanup and operations at document level.
Each ``parser'' focuses on specific extractions and, internally, could combine multiple machine learning (ML) models and rule-based (RB) algorithms.
% The RB algorithms are implemented a SpaCy (\url{https://spacy.io} component called ``entity ruler''.
The ML models are available in multiple architectures which can be plugged in and out with just a configuration change. 
The conditional Random Field (CRF) is the default architecture and it's implemented via the Wapiti~\cite{lavergne2010practical} library. It's by far the fastest (token/seconds) implementation available in Grobid. 
Deep learning models are interfaced with DeLFT (Deep Learning For Text)~\cite{DeLFT} which offers a plethora of models and architectures. 
For sequence labelling, the architectures obtaining the best results are the Bidirectional LSTM with a CRF as activation layer (BidLSTM\_CRF), a slight variation, the BidLSTM\_CRF\_FEATURE allows an additional input channel to pass orthogonal features. 
Finally, DeLFT also supports the transformers architecture through BERT or SciBERT. 

\begin{figure}[ht]
\includegraphics[width=\textwidth]{extraction-schema-3}
\caption{\label{fig:extraction-ml-models-cascade-architecture} Cascade architecture and interaction between the superconductors, and the material parser. The rectangles indicate the information described in Table~\ref{tab:superconductors-parser-entities} and~\ref{tab:material-parser-entities}. }
\end{figure}

Figure~\ref{fig:extraction-ml-models-cascade-architecture} illustrates the data flow with the interaction between parsers and the extracted entity types. 

\paragraph{Superconductors parser}
Combines two ML models: \textit{superconductors} ML model and \textit{quantities} ML model.
%Combines two ML models (\textit{superconductors} ML model and \textit{quantities} ML model) and two RB extractors (\textit{crystal structures type} entity ruler and \textit{space groups} entity ruler). 
The \textit{superconductors} model is trained trained using SuperMat~\cite{foppiano2021supermat} and aims to extract the main entities from the superconductors domain (evaluation in Table~\ref{tab:evaluation-10fold-superconductors-parser}).
We provide models trained using Conditional Random Field (CRF), Recurrent Neural Networks (RNN) using Bidirectional LSTM with CRF~\cite{Lample2016NeuralAF}, and Transformers (SciBERT~\cite{Beltagy2019SciBERT}).
The \textit{quantities ML model} is a model specialised in extracting quantities of measurements from the \textit{grobid-quantities}~\cite{foppiano2019quantities} project and trained on scientific text.
We filter entities extracted by the \textit{quantities} model to retain only temperatures and pressures and we merge them with the entities extracted by the \textit{superconductors ML model} to remove duplicates or keep the largest matches.
%Finally, the rule-based components are extracting the structural information such as the crystal structure type (e.g NaCl-type), the space groups and the unit cell type. 
%We use rule based because the number of items is finite with lower variation.  
The extracted entities are summarised in Table~\ref{tab:superconductors-parser-entities}. More details about the definition of the machine learning entities can be obtained in the work on SuperMat~\cite{foppiano2021supermat}.

\emph{\textbf{Discussion on the machine learning implementation}}
% Discussion on the ML, transformers, TPU etc.... 
The \textit{superconductors} model training using SuperMat is composed by two main component, the annotated text and the orthogonal features which are captured by processing PDF documents.
The features names and description is summarised in Table~\ref{tab:superconductors-ML-model-features}. 
We have three classes of features: a) features calculated from the single token, which are the classical features inherited from Grobid, b) layout features uniquely extracted from the PDF structure such as the font name or style, and c) features extracted using external services. 
We use ChemDataExtractor~\cite{chemdataextractor1} to identify whether a token is a chemical compound, this helps to improve the recognition of materials formulas. 

\begin{table}[ht]
\centering\small
\scalebox{0.71}{
\begin{tabular}{m{8em} m{42em}}
\toprule
\textbf{Name} & \textbf{Description} \\ 
\midrule
\multicolumn{2}{c}{\textbf{Computed from the token}} \\
\midrule
capitalisation & Indicate whether the token starts with an uppercase letter (INITCAP), is fully uppercase (ALLCAPS), or does not contains any uppercase letters (NOCAPS)\\
digit & Indicate whether the token is composed only by digits (ALLDIGIT), contains some digits (CONTAINDIGIT), or does not contains any digits (NODIGIT)\\ 
singleChar & Indicate whether the token is composed by a single character \\ 
punctType & Is used to indicate the presence of absence of punctuation and to normalise the different characters for the same symbol under a single indication: no punctuation (NOPUNCT), open or end brackets (OPENBRACKET, ENDBRACKET), various punctation (DOT, COMMA, HYPHEN, QUOTE)  open or close quotes (OPENQUOTE, ENDQUOTE), and a default value for everything else (PUNCT).\\
shadowNumber & Compute the numbers by shadowing them replacing them with an ``X''\\
wordShape & replace each character with a mark: ``x'' for lowercase letters, ``X'' for uppercase letters, and ``d'' for digits. \\
wordShapeTrimmed & like wordShape but compresses the mask by aggregating equals subsequent values\\
\multicolumn{2}{c}{\textbf{Extracted from PDF documents}} \\
\midrule
fontStatus  & Indicate the font as extracted from the PDF document \\
fontSize & Indicate the size of the font as extracted from the PDF document\\
fontStyle & Indicate whether the font is standard (BASELINE), superscript (SUPERSCRIPT) or subscript (SUBSCRIPT)\\
bold & the token's font is in bold \\
italic & the token's font is in italic\\
\multicolumn{2}{c}{\textbf{Computed by external services }} \\
\midrule
chemicalCompound & indicate the token to be identified as a chemical compound by ChemDataExtractor\cite{chemdataextractor1} \\
\bottomrule
\end{tabular}
}
\caption{Summary of the features utilised by the superconductors ML model. }
\label{tab:superconductors-ML-model-features}
\end{table}
---


[Optional TODO]: Discussion about the positive sampling and experiments about negative and active sampling. 

[Optional TODO]: Add discussion about the GloVE emvbeddings experiments (PO) - if positive results

[Optional TODO]: Add discussion about the TPU training - if positive results

\begin{table}[ht]
\centering\small
\scalebox{0.71}{
\begin{tabular}{m{19em} m{30em}}
\toprule
\textbf{Entity} (\textbf{Tag})& \textbf{Description} \\ 
\midrule
\multicolumn{2}{c}{\textbf{Machine learning}} \\
\midrule
Material (\texttt{<material>}) & Materials and samples names, formulas, including stochiometric formulas, substitution variables of values and elements, shape, doping, substrate \\
Class (\texttt{<class>}) & Groups of materials having similar characteristics or common strategic compounds that define their nature \\
T\textit{c} value (\texttt{<tcValue>})& The value of the superconductors critical temperature\\
T\textit{c} expressions (\texttt{<tc>}) & Expressions in the text that provide information about the phenomenon of superconductivity related to a value, interval or variation of the T\textsubscript{c}\\
Measurement methods (\texttt{<me\_method>}) & Techniques used to measure or calculate the presence of superconductivity. \\
Applied pressure (\texttt{<pressure>}) & Applied pressure when superconductivity is recorded\\
\bottomrule
% \hline
% \multicolumn{2}{c}{\textbf{Rule-based}} \\
% \hline
% Crystal structure type (\texttt{<crystal-structure>}) & Geometry of arrangement of particles in the unit cell\\
% Space groups (\texttt{<space-group>}) & Symmetry group, usually in three dimensions\\
% Unit cells type (\texttt{<unit-cells>}) & A repeating unit formed by the vectors spanning the points of a lattice\\
% \hline
\end{tabular}
}
\caption{Synthesis of the superconductors parser entities. }
\label{tab:superconductors-parser-entities}
\end{table}

\paragraph{Material parser} is used to further structure the \texttt{<material>} entities from the \textit{Superconductors parser}. 
First, the material raw string is passed through a \textit{material ML model} to extract several attributes, summarised in Table~\ref{tab:material-parser-entities}. 
This model is trained using annotated material raw strings (Table~\ref{tab:evaluation-10fold-material-parser}). 
We provide models trained using CRF and RNN.
%Since the performances with CRF or RNN are above 90\%, we do not supply the model based on transformers for this task. 
Then, we apply several processing based on which information are available: 
\begin{itemize}
    \item we decompose the formula into a structured composition and we identify each group of element-stoichiometry (e.g ``O'': 7.0) using a combination between mat2chem~\cite{kononova_text-mined_2019} and Pymatgen~\cite{Ong2013}, 
    \item we obtain the formula corresponding to the material name, if the name available and the formula is not (e.g. hydrogen to \textit{H}), 
    \item we calculate the classes from the formula composition, using a simple algorithm we have designed which can return values such as Cuprate, Oxides, Alloys, etc.
    \item using the extracted variables and values we apply permutation to the stochiometric formula to resolve the substitutions. For example the stochiometric formula \texttt{La 4 Fe 2 A 1-x O 7 (A=Mg,Co; x=0.1,0.2)} is substituted for the permutation of the values of A and x to become \texttt{La 4 Fe 2 Mg 0.9 O 7}, \texttt{La 4 Fe 2 Mg 0.8 O 7}, \texttt{La 4 Fe 2 Co 0.9 O 7}, \texttt{La 4 Fe 2 Co 0.8 O 7}.
\end{itemize}

\begin{table}[ht]
\centering
\scalebox{0.7}{
\begin{tabular}{m{16em} m{30em}}
\toprule
\textbf{Entity} (\textbf{Tag})& \textbf{Description} \\ 
\midrule
Name (\texttt{<name>}) & The canonical name of a material (e.g. Hydrogen, PCCO, Carbon) \\
Formula (\texttt{<formula>}) & Chemical formula of the material (e.g. \texttt{Pr1.869Ce0.131CuO 4-}, \texttt{MgB2}, \texttt{La 2-x Sr x CuO 4}) \\
Doping (\texttt{<doping>})& Doping ratio and doping materials that are adjoined to the material name (e.g. \texttt{Zn-doped}, \texttt{2\% Zn-doped})\\
Shape (\texttt{<shape>}) & shape of the material (e.g. single crystal, polycrystalline, thin film, powder, film) \\
Substitution variables (\texttt{<variable>}) & Variables that can be substituted in the formula. \\
Substitution values (\texttt{<value>}) & Values expressed in the stoichiometric doping. \\
Substrate (\texttt{<substrate>}) & Substrates as defined in the material name \\
Fabrication (\texttt{<fabrication>}) & Represent eventual additional information that are not belonging to any of the previous tags  (e.g. intercalated, electron-doped)\\
\bottomrule
\end{tabular}
}
\caption{\label{tab:material-parser-entities} Synthesis of the material parser entities. }
\end{table}

The features used in the \textit{material} ML model are limited to the one "computed from the token" as described in Table~\ref{tab:superconductors-ML-model-features}. 

\paragraph{Post-processing} The post-processing is the final step of the extraction and has the objective of running aggregations at document level. 
We utilise the decomposed formulas to aggregate the mention in the document that is referring to the same material. 
For example we could aggregate simple cases such as \texttt{hydrogen} and \texttt{H} but also a complex stochiometric formula (e.g. \texttt{La 2 Fe 1-x O 7 (x = 0.1, 0.2)} with formulas discussed further in the paper. 

\paragraph{Evaluation}

In this section we discuss the machine learning evaluation scores for the superconductors and materials ML models. The evaluation was conducted using the 10-fold cross validation to obtain precision, recall and f1 score. The overall scores were calculated using the micro average which compensate the different balance of entities annotations. 

The superconductors model is detailed in Table~\ref{tab:evaluation-10fold-superconductors-parser} and obtained the best result using transformers, in particular SciBERT had a F1 score of 83.46\% and recall around 85\%. Both deep learning approaches (SciBERT and RNN) showed most of the gains in the recall.
We noticed that the CRF approach had more difficulty to extract large entities which were partially extracted. 
In term of performances, the CRF is the fastest approach and the SciBERT is the slowest, limited by the large neural network that needs to be loaded in memory.
Overall, the BidLSTM+CRF (RNN) had the best trade-off between performances and throughput, as compared with the SciBERT approach. 

We notice that \texttt{<pressure>} results in lower performances and the reason due to the lack of enough training data as compared with the rest of the labels. 

\begin{table}[ht]
\centering\small
\scalebox{0.73}{
\begin{tabular}{lrrrrrrrrr}
\toprule
& \multicolumn{3}{c}{\textbf{CRF}} & \multicolumn{3}{c}{\textbf{BidLSTM+CRF} (RNN)} & \multicolumn{3}{c}{\textbf{SciBERT} (transformers)}\\ 
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
\textbf{Label} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
\texttt{<class>}         & 79.69 & 75.54 & 77.55 & 81.84 & 83.96 & 82.85 & 79.58 & 85.79 & 82.56\\
\texttt{<material>}      & 82.9  & 81.33 & 82.1  & 85.18 & 83.86 & 84.51 & 83.89 & 86.13 & 84.99\\
\texttt{<me\_method>}    & 82.47 & 81.26 & 81.84 & 83.51 & 83.37 & 83.43 & 83.92 & 86.50 & 85.19\\
\texttt{<pressure>}      & 65.03 & 54.01 & 58.26 & 63.79 & 73.24 & 67.98 & 63.92 & 71.18 & 67.27\\
\texttt{<tc>}            & 84.63 & 80.73 & 82.63 & 83.70 & 81.66 & 82.66 & 80.91 & 83.00 & 81.94\\
\texttt{<tcValue>}       & 79.3  & 74.95 & 76.97 & 73.23 & 80.73 & 76.76 & 76.74 & 85.00 & 80.65\\
\midrule
All (micro avg) & 82.43 & 79.68 & 81.03 & \textbf{83.01} & 82.89 & 82.95 & 83.01 & \textbf{85.06} & \textbf{83.46}\\
\bottomrule
\end{tabular}
}
\caption{\label{tab:evaluation-10fold-superconductors-parser} Evaluation scores for the superconductor models using 10-fold cross-validation. }
\end{table}

The material parser is provided in two architectures: CRF and BidLSTM+CRF because the performances are above 95\% and the SciBERT approach is very slow. 
Surprisingly, here the CRF performs than the RNN. We notice that \texttt{<fabrication>} and \texttt{<substrate>} have relatively low results, for the same reason of \texttt{<pressure>} of lack of enough training data.


[PO we can potentially extend this part adding an additional section discussing NER]

\begin{table}[ht]
\centering\small
\begin{tabular}{lrrrrrr}
\toprule
& \multicolumn{3}{c}{\textbf{CRF}} & \multicolumn{3}{c}{\textbf{BidLSTM+CRF} (RNN)}\\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
\textbf{Label} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
\texttt{<doping>}      & 86.31   & 86.2     & 86.22 & 84.87 &  87.82 & 86.29   \\
\texttt{<fabrication>} & 60.93   & 50.3     & 54.27 & 7.43  &  13.33 & 8.94   \\
\texttt{<formula>}     & 98.06   & 97.93    & 98    & 97.33 &  97.51 & 97.42  \\
\texttt{<name>}        & 97.25   & 96.95    & 97.09 & 96.08 &  96.70 & 96.38  \\
\texttt{<shape>}       & 97.26   & 97.15    & 97.2  & 96.38 &  94.93 & 95.64  \\
\texttt{<substrate>}   & 83.33   & 60.83    & 68    & 65.76 &  97.50 & 77.66  \\
\texttt{<value>}       & 95.99   & 95.99    & 95.99 & 95.28 &  95.84 & 95.56  \\
\texttt{<variable>}    & 99.24   & 98.85    & 99.04 & 98.94 &  98.99 & 98.96  \\
\midrule
all  (micro avg)       & 96.87   & 96.51    & 96.68  & 95.75 &   96.44  &  96.09  \\
\bottomrule
\end{tabular}
\caption{\label{tab:evaluation-10fold-material-parser} Evaluation scores of 10-fold cross-validation of the material parser. }
\end{table}


\subsubsection{Linking}

%Introduction of the linking
The Linking is the final step of the processing that aims to link together entities previously extracted.
%Objective of the linking
We can formalised it as follows. \textit{Given a text T and two or more entities e\textsubscript{1}...e\textsubscript{n} of two types t\textsubscript{1} and t\textsubscript{2}, determine links between entities of type t\textsubscript{1} can be linked to entities of type t\textsubscript{2} .} 

%We have experimented several options: dependency parsing, rule-based, and sequence labelling. 
Our implementation uses a main rule-based method and a secondary approach based on sequence labelling. Other approaches, such as the use of dependency parsing based on different proposed methods~\cite{yoshikawa:2017acl, Tiktinsky2020pyBARTES, swayamdipta:17, zhou-zhao-2019-head} were studied and discarded because we found that a) it is difficult to find a suitable parser for scientific texts, and b) complementary methods based on complex rule sets are needed to compensate for the poor performance of the parser.

In our implementation we focus on three types of relationships: 
\begin{itemize}
    \item \textbf{material-tcValue} links a material with its corresponding superconducting critical temperature value, 
    \item \textbf{tcValue-pressure} connects a superconducting critical temperature value with its related critical pressure, and 
    \item \textbf{me\_method-tcValue} connects the superconducting critical temperature value to its corresponding measurement method.
    % \item \textbf{material-crystal\_structure} link the material with their crystal structure, and 
    % \item \textbf{material-space\_group} to link the material to their space groups.
\end{itemize}

% Algorithm in brief (three or four different scenario): 
\paragraph{T\textsubscript{c} classification}
For linking the T\textsubscript{c} to any other entity type, there is a classification engine for deciding whether the temperature entity is  ``a superconducting critical temperature'' or not. 
The classification is rule-based and combines the extracted entities of T\textsubscript{c} expressions (label \texttt{<tc>}) with a set of predefined standard names. 
When a T\textsubscript{c} is not considered a ``superconducting critical temperature'' it is excluded from the list of possible linking candidates. 

\paragraph{Rule-based linking}
The rule-based approach works with two main scenarios: a) if entities to be linked have cardinality one in the sentence, they are linked automatically, and b) when their cardinality is higher then they are linked ``by distance'' or ``in order'', depending on the structure of the sentence. 
If the word \textit{``respectively''} appears in the sentence, entities are linked ``in order'', otherwise they are linked ``by distance''. 
For example, the sentence:  
\begin{displayquote}
P-or Ba-122  and Co-doped Ba-122 have lower T c s of about 30 K and 24 K, respectively, which makes helium free operation questionable.
\end{displayquote}
containing the word ``respectively'' and therefore is processed by linking the materials in order: \textit{P-or Ba122} is assigned to \textit{30 K} and \textit{Co-doped Ba-122} to \textit{24 K}.
The ``in order'' method is very sensible to missing entities, for example if one entity is not extracted by mistake it will lead to incorrectly assign all the entities in the block. To avoid this, if the entities are unbalanced we reduce the search space as follows: 
\begin{itemize}
    \item m entities of type\textsubscript{1} vs n entities of type\textsubscript{2} with $m > n$: we shift the starting point to start at the $m - $n entity 
    \item m entities of type\textsubscript{1} vs n entities of type\textsubscript{2} with $m < n$: we shift the ending point at the n\textsubscript{-1} entity
\end{itemize}
With this sentence structure we know that the entities are in order, however if one is missing we just ignore the entities in surplus without disrupting the assignment. 

We define the distance measurement \textit{d} as a value calculated in numbers of characters from the centre of the entities. 
If the entity is surrounded by parenthesis we expand it to the whole parenthesis and centre is shifted and all its content refers to the same entity. 
As an example, in the sentence
\begin{displayquote}
We tested two materials MgB2 (Tc = 39 K) and FeSe (Tc = 16 K).
\end{displayquote}
both temperatures entities are expanded to their containing parenthesis e.g. \texttt{39 K} to \texttt{(Tc = 39 K)} in this case moving the centre, in this case the centre was shifted from 38 a 35 toward the left.
As a result, the distance tend to be shorter to entities immediately closed to the whole parenthesis block and the impact of the dimension of the whole parenthesis block is reduced when calculating the distance (the entity could be on each side within the parenthesis). 

The distance calculation is also adjusted with the addition of ``penalties'', that is, the distance is doubled when certain keywords such as ``,'', ``.'', ``;'', ``and'', ``but', ``while', ``whereas', ``which'', ``although'' representing logical separation of predicates appear in the sentence.~\cite{oka2021table} The rationale is that element between two separated predicates are likely not to be linked although they might be at a smaller distance. 

The rule-based linking is evaluated using the linked entities from SuperMat~\cite{foppiano2021supermat} (Table~\ref{table:evaluation-linking}). 
Each task aims evaluating the linking between two entities types, assuming a 100\% accuracy in the Extraction step. 
The result of the \texttt{material-tcValue} indicates F1 score >80\% with a precision of 88\%.

\begin{table}[ht]
\centering\small
\begin{tabular}{lcccc}
\toprule 
\textbf{Link type} (Method) & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & Support (avg) \\ 
\midrule
\textbf{material-\tc} (RB)      &  88.40    & 74.52 &    80.87 &   726  \\
\textbf{\tc-pressure} (RB)      & 85.71  &  71.52  &  77.98  &  118     \\
\textbf{\tc-me.method} (RB)     & 62.28 & 65.74 &  63.96  &  151 \\
% \textbf{material-\tc} (CRF)    & 68.52 &	70.11   &  69.16  &  \\
% \textbf{\tc-pressure} (CRF)    & 72.92 &	67.67   &  69.76  &  \\
% \textbf{\tc-me.method} (CRF)   & 49.99 &	45.21   &  44.65  & \\
\bottomrule
\end{tabular}
\caption{\label{table:evaluation-linking} Evaluation scores of the linking methods. }
\end{table}

\paragraph{Sequence labelling linking} We designed the linking based on sequence labelling as a simple task where each token can be assigned two labels: \texttt{<link\_left>} or \texttt{<link\_right>}. 
Furthermore, a decoder processing the labelled sequence is assigning links to entities.
We trained three models, one model for each link type (or entity class pair), using SuperMat~\cite{foppiano2021supermat}.
The evaluation was performed using 10-fold cross-validation (Table~\ref{tab:evaluation-crf-linking-cross-validation}) with F1 score of 69.16\% for material-tcValue, 44.65\% for tcValue-pressure and 69.66\% for tcValue-me\_method. 

[TODO: add evaluation considering pairs of right-left, which should be lower :-(]
\begin{table}[ht]
\centering\small
% \scalebox{0.7}{
\begin{tblr}{lrrrr}
\hline[0.75pt,solid]
\textbf{Label} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support}\\ 
\hline
\multicolumn{5}{l}{\hspace{1em}\emph{material-tcValue}} \\
\hline[dashed]
\texttt{<link\_left>}  & 70.78   & 73.97 & 72.1  & 505    \\
\texttt{<link\_right>} & 66.26   & 66.26 & 66.12 & 507    \\
\hline
all (micro avg.)       & 68.52   & 70.11 & 69.16 &       \\
\hline
\multicolumn{5}{l}{\hspace{1em}\emph{tcValue-me\_method}} \\
\hline[dashed]
\texttt{<link\_left>}  & 50.67   & 58.08 & 51.1  & 40    \\
\texttt{<link\_right>} & 44.71   & 31.83 & 35.68 & 39    \\
\hline
all (micro avg.)       & 49.99   & 45.21 & 44.65 &       \\
\hline
\multicolumn{5}{l}{\hspace{1em}\emph{tcValue-pressure}} \\
\hline[dashed]
\texttt{<link\_left>}  & 76.48   & 67.78  & 70.66 & 113    \\
\texttt{<link\_right>} & 71.02   & 67.57  & 68.89 & 117    \\
\hline
all (micro avg.)       & 72.92   & 67.67  & 69.76 &       \\
\hline[0.75pt,solid]
\end{tblr}
% }
\caption{Evaluation of the CRF-based linking using 10-fold cross-validation for the three linking models. }
\label{tab:evaluation-crf-linking-cross-validation}
\end{table}

This approach has a several limitations which impact effectiveness in training and tagging. 
Firstly, since the sequence is flat, (a) it is limited to pairs of adjacent entities. Secondly, (b) the decoding of the annotated sequence is challenging and the linking will not be successful if the sequence of ``right-left'' labels goes out of sync e.g. if the model tags two subsequent entities with the same label one needs to be ignored. Finally, (c) this architecture limit the benefit of the training data, because it supports only certain type of sequences with adjacent entities. The model for tcValue-me\_method is particularly impacted by this limitation because the method of measurement is discussed often far from the actual results are illustrated. 
[OPTIONAL TODO: add study on how we could create more training data, for example by duplicating the same sentence and picking up different entities]


\subsection{End to end evaluation}

% What is the end 2 end evaluation? 
The end to end evaluation (E2EE) measures the real capacity of the system from document to database.
We limit the scope of the E2EE to the pair `material-Tc' and, when available, the triplet `material-Tc-pressure' which, at the moment, are the backbone on which the database is built upon.
We perform the E2EE on a dataset composed by a previously selected set of 500 documents (500-papers) from three publishers: American Institute of Physics (AIP), American Physical Society (APS) and Institute of Physics (IOP)~\cite{foppiano2019proposal}.
We manually examine and correct the resulting mini-database, a) marking records that are invalid and b) identify the cause of failure from a predefined set of five error types, described below and illustrated in Figure~\ref{fig:error-types}. 
\begin{itemize}
    \item \textbf{From table} indicate when the extracted text is wrongly extracted from a table 
    \item \textbf{Extraction} indicate when the failure was caused by entities not recognised, or wrongly recognised (e.g. partially truncated)
    \item \textbf{Quantities extraction} when the entities of type quantities (pressure, temperature) are not correctly extracted. This is separated because such entities are obtained by two different models 
    \item \textbf{Tc classification} when the temperature is wrongly classified as Tc
    \item \textbf{Linking} when, given the all above items were correct, the entities were not linked correctly. 
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{error-types-2.png}
\caption{Error types for the end to end evaluation described in the context of the data flow. }
\label{fig:error-types}
\end{figure}

The E2EE is summarised in Table~\ref{table:end2end-evaluation-summary}. 
We present only precision because is the metric we want to maximise. Recall is omitted because less relevant and difficult to calculate manually. 
We show the metrics by Title, Abstract, Paragraph, Figure caption, Unknown subsections and we provide average scores using micro-average. 
The system scores 72.60\% considering all the subsections. 
We notice there is a clear difference between error rates of Figure, caption and unknown sections with the other subsections. 
One solution to improve correctness, is to exclude text from certain subsections. 
The scores increase to 73\% when excluding unknown sections, to 75.24\% when excluding figure captions, and 79.14\%  when both. 
Both figure captions and unknown sections count for less then 20\% of the total number of sections. 


\begin{table}[ht]
\centering\small
\begin{tabular}{l c c}
\toprule
\textbf{Subsection} & \textbf{Precision} & \textbf{Support} \\ 
\midrule
Title               & 100       & 2     \\
Abstract            & 80.32     & 61    \\
Paragraph           & 75.2      & 623   \\    
Figure captions     & 59.28     & 140   \\    
Unknown             & 57.14     & 21    \\
\midrule
\textbf{Micro avg.}  & 72.60     & 847   \\
\textbf{Micro avg.} (excl. figures)  & 75.24     & 707   \\ 
\textbf{Micro avg.} (excl. unknown sections)  & 73.00     & 603   \\ 
\textbf{Micro avg.} (excl. figures and unknown sections)  & 79.14     & 657   \\ 
\bottomrule
\end{tabular}
\caption{Evaluation end to end: summary of the scores. }
\label{table:end2end-evaluation-summary}
\end{table}

The error types are summarised in Figure~\ref{fig:error-types-distribution}. The most common failures are originated by Tc classification (40\%), Linking (32\%), and Extraction (20\%).

\begin{figure}[ht!]
\centering
\includegraphics[width=0.7\textwidth]{error-type-distribution-image}
\caption{Error type distribution. }
\label{fig:error-types-distribution}
\end{figure}


The most commons Tc classification's failures are as incorrect recognition of: a) relative values of \tc, $\Delta$\tc, values indicating the transitional temperature width,  b) temperature values that are not \tc for example they are just temperatures T, T\textsubscript{Curie}, and c) values of temperature where there is absence of superconductivity.
The errors of type Linking are divided mainly with a) when the authors are comparing relative values of Tc or absolute values of materials (e.g. \textit{The Tc = 12 K is similar to the one of Mg B2}), b) the material entity is not the subject of the \tc  in the sentence. 
Finally, Extraction issues are mainly: a) the recognised entities does not represent a material but an annealing product, a machine, b) implicit mention of the main material when experimented with different substrates, and c) mismatches between material and class. 

% \begin{table}[ht]
% \centering
% \begin{tabular}{lcc}
% \hline \textbf{Error type} & \textbf{Rate} & \textbf{Support} \\ 
% \hline
% From table              & 2.58  & 6     \\
% Extraction              & 20.25 & 47    \\
% Quantities extraction   & 3.44  & 8     \\
% Composition resolution  & 1.29  & 3     \\
% Tc classification       & 40.08 & 93    \\
% Linking                 & 31.89 & 74    \\
% \hline
% \hline
% \textbf{Total}  & & 231   \\
% \hline
% \end{tabular}
% \caption{Evaluation end to end: error types. }
% \label{table:end2end-evaluation-errur types}
% \end{table}


\section{Supercon\textsuperscript{2}}

We created SuperCon\textsuperscript{2} by processing 37770 research papers belonging to the category \textit{cond-mat.supr-cond} in ArXiv. 
Currently SuperCon\textsuperscript{2} contains 40324 records including 2052 triplets \textit{material-Tc-applied pressure}, and 3602 records with explicit measurement method \textit{material-Tc-measurement method}.
In addition, the schema contains additional information which are described in Table~\ref{tab:superconductors-parser-entities} and~\ref{tab:material-parser-entities} and richer bibliographic data described in Section~\ref{subsubsec:document-structuring}. 

The schema of Supercon\textsuperscript{2} is summarised in Table~\ref{tab:supercon2-schema}. The fields are grouped in four main groups: material information, properties, document bibliographic information, and internal information.
Material information contains the characteristics of the extracted material. 
Properties groups the target properties and conditions that we are aiming to extract. 
Document bibliographic information are related to the original paper. 
Internal information are private information that are used for the correct functioning of the SuperCon\textsuperscript{2} interface. 



\begin{table}[ht]
\centering\small
\begin{tabularx}{\linewidth}{p{50}XX}
\toprule 
\textbf{Field name} & \textbf{Description} & \textbf{Examples} \\ 
\midrule

\multicolumn{3}{c}{\emph{Material information}} \\
Raw material & The material or sample as it appears in the text &\\
Name  & Represent the canonical name of a material & PCCO, PCO, Metal diboride, hydrogen, carbon \\
Formula & material expressed as chemical formula. This includes also formulas with stochiometric variables & $Pr1.869Ce0.131CuO 4-\delta$, MgB2, La 2-x Sr x CuO 4 \\
Doping  & Doping ratio and doping materials that might be adjointed to the material & overdoped, underdopded, optimally doped, bulk, pure, 1\% Zn, Zn (from Zn-doped XYZ)\\
Shape  & The shape of the material or the sample & single crystal, polycrystal, wire, powder, film \\
Variables  & variables that can be substituted in the formula & x = 0, RE=Ln,St\\
Class  &&\\
Fabrication  & All the information that are not belonging to any of the previous tags &  intercalated, synthesized by MBE method, electron-doped, hole-doped \\
Substrate  & Substrate material described in the raw material & PCCO films onto Pr 2 CuO 4 (PCO)/SrTiO 3 \\

\multicolumn{3}{c}{\emph{Properties}} \\
Critical temperature  & Superconducting critical temperature &\\
Applied pressure  & Pressure applied when measuring the superconducting critical temperature&\\
Measurement Method  & Method for measurement of the superconducting critical temperature& Magnetic susceptibility, specific heat, calculation, prediction, resistivity\\

\multicolumn{3}{c}{\emph{Document bibliographic information}} \\
Section & The main body section of the paper& header, body, annex\\
Subsection & The secondary segmentation area of the paper& paragraph, table caption, figure caption, title, abstract \\
DOI, Authors, Title, Publisher, Journal, Year & \multicolumn{2}{l}{The respective bibliographic information}\\
\multicolumn{3}{c}{\emph{Internal information}} \\
Hash, Timestamp & \multicolumn{2}{l}{The Hash of the binary content of the original PDF and the timestamp of the upload.} 
\bottomrule
\end{tabularx}
\caption{\label{tab:supercon2-schema} Summary and description of the Supercon\textsuperscript{2} schema. }
\end{table}

\subsection{Ingestion workflow}
\label{subsec:ingestion-workflow}

The ingestion workflow is performed with two processes (Figure~\ref{fig:ingestion-workflow}): ``Extraction process'' to transform the PDF documents to their JSON representation which follows the schema described in Section~\ref{subsubsec:document-structuring}, and ``Aggregation process'' to compress the document information in a synthesised table (CSV or TSV) format.
We designed the ingestion workflow so that processes can run at the same time and allowing processing documents and records in parallel based on service capacity. 
The workflow accesses \textit{grobid-superconductors} via asynchronous call to REST API allow decoupling between the workflow and the application and gives the flexibility to add more instances at ease.
The workflow is orchestrated using a persistence layer implemented with MongoDB to store information and metadata such as error and failures.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{workflow-schema-1}
\caption{Schema of the ingestion workflow}
\label{fig:ingestion-workflow}
\end{figure}

The persistence layer is organised in four main collections: 1) binary collection containing the original PDF documents, 2) ``document'' collection with the JSON documents (the schema is described in Section~\ref{subsubsec:document-structuring}) obtained from the Extraction task, 3) ``tabular'' collection with the distilled record information described in Table~\ref{tab:supercon2-schema}, and 4) a logger collection with the status and error code for each processed document or record. 

\subsection{User interface}
\label{sucsec:supercon2-user-interface}
SuperCon\textsubscript{2} can be utilised as text file (CSV or TSV) for processing and to build ML models and data-driven applications. 
Although the automatic system might achieve good results and extract a large quantity of data, we are aware that some sort of ``user validation'' is required for improving the data quality. For this reason we have built SuperCon\textsubscript{2} with in mind the needs of an efficient curation interface. 
Currently, the interface provides a) access and exploration of tabular data with navigation, filters, and exporting in various formats including CSV, TSV, Microsoft Excel (Figure~\ref{fig:interface-supercon2}), and b) visualisation of the PDF documents enhanced with the extracted information Figure~\ref{fig:pdf-annotations}). 

\begin{figure}[ht]
\includegraphics[width=\textwidth]{sample-database-3}
\caption{\label{fig:interface-supercon2} Example of SuperCon\textsubscript{2} database interface}
\end{figure}

The PDF visualisation allows the user to rapidly find the extracted entities and examine their content, for example, given a material, is possible to know to which properties it was linked.
Materials attributes (discussed in Section~\ref{subsubsec:extraction}) are visible in a box that appear on the side of the document. 

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{sample-pdf-annotations}
\caption{\label{fig:pdf-annotations} Example of superconductors research document~\cite{sample_superconductors_article} with enhanced annotations}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
In this work we present our solution for automatically creating a database of materials and properties from scientific literature. 
We designed our solution into three components: a) \textit{grobid-superconductors}, a system that combines NER and EL for converting PDF documents into tabular data. b) an ingestion workflow which exploit the multiprocessing and client-server architecture to process large quantities of data and store it in a persistence layer. c) An interface which combine tabular navigation and native annotation on PDF documents to provide means for rapid data exploration. 
Using our solution, we processed 37700 scientific articles and obtained SuperCon\textsubscript{2}, a database with 40324 records of superconductors materials and properties including the applied pressure and the T\textsubscript{c} measurement method which are currently not available in SuperCon. 
SuperCon\textsubscript{2} is available in text format at \url{https://github.com/lfoppiano/supercon}.

In future we will continue developing our applications by: a) extracting more properties such as crystal structure type, space groups type and lattice structure, b) studying supervised methodologies for the Linking step to replace the CRF implementation, and c) extend the interface to support more advanced and efficient curation.


\section{Acknowledgement}
\label{sec:acknowledgement}
Our warmest thanks to Patrice Lopez, the author of Grobid (\url{https://github.com/kermitt2/grobid}), DeLFT (\url{https://github.com/kermitt2/delft}) and other interesting TDM open-source projects.

\bibliography{bibliography}
\bibliographystyle{plain}


\end{document}
