\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{authblk}
\usepackage{csquotes}
\usepackage{array}
\usepackage{amssymb}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}


\title{Automatic Extraction of Materials and Properties from Superconductors Scientific Literature}
\author[1]{Luca Foppiano}
\author[2]{Pedro Baptista de Castro}
\author[3]{Pedro Ortiz}
\author[4]{Laurent Romary}
\author[2]{Kensei Terashima}
\author[2]{Yoshihiko Takano}
\author[1]{Masashi Ishii}

\affil[1]{Material Database Group, MaDIS, NIMS, Tsukuba, Japan}
\affil[2]{Nano Frontier Superconducting Materials Group, MANA, NIMS, Tsukuba, Japan}
\affil[3]{Data and Web Science Group, University of Mannheim, Germany}
\affil[4]{ALMAnaCH, Inria, Paris, France}

\begin{document}

\maketitle

\begin{abstract}
The automatic extraction of materials and related properties from the scientific literature is getting more attention in data-driven materials science (Materials Informatics). 
We present our system for automatically extract superconductor material names and respective properties from PDF documents.
Our system combines Machine Learning and Heuristic approaches using a flexible architecture: developers can plug-in various Named Entities Recognition (NER) methods (CRF, BidLSTM, Transformers) or relation extraction approaches (ruled-based, CRF).
We have processed X papers and extracted Y materials and properties: superconducting critical temperature, crystal structure and, when available, applied pressure and measurement method.
The data obtained is suitable as input in newly material discovery projects and is available in machine-readable format.
\end{abstract}

\keywords{material informatics, superconductors, machine learning, nlp, tdm}

\section{Introduction}
% 1. Start about SC, introduce few applications (Stanev), most of SC of discovery by "feeling" or "serendipidity"
The research of new superconducting materials is still an hot topic in both fundamental science and practical applications.
Superconductors display many interesting quantum phenomena including persistent electrical currents, zero-resistivity, ability to host a strong magnetic field, vortex pinning, and quantisation of the magnetic flux. [add references]
Moreover, there is a growing list of practical and theoretical applications of superconductors including medical instruments (MRI/MNR), high-speed trains, quantum computers, and components of particles accelerators, such as the Linear Hadron Collider (LHC)~\cite{PhilippeBook, Kizu2010ConstructionOT, Cardani2017NewAO}.
However, discovering a new superconductor is still a challenging task~\cite{PhysRevB.103.014509, doi:10.1088/1468-6996/16/3/033503} because the phenomena is not yet fully understood and the relation between superconductivity and chemical composition remains unclear. 
Most of the superconductors materials have been discovered using the "feeling" (or "serendipity") approach following researchers intuition rather than any particular methodology.~\cite{doi:10.1080/08957959.2019.1695253}

% 2. Use of data-driven approaches is becoming more popular and fundamental for material discovery (there are many reviews - example citations data from Pedro) 
% ~\cite{stanev_machine_2017, le2020critical,Hamlin2019SuperconductivityNR}.(Superdocutivity releated citations)

% Pedro
In the recent years, with the creation of computational databases, such as the Materials Project (MP)~\cite{materialsprojectJain2013}, the Open Quantum Materials Database (OQMD)~\cite{oqmdkirklin2015open}, and then experimental data repositories such as NIMS MDR (\url{http://mdr.nims.go.jp})~\cite{ranganathan_anusha_2019_3553963}, the focus has been steadily shifting towards a data-driven design of materials. In this new paradigm, the efficient use of data to guide experiments and materials property prediction through use of machine learning methods takes the centre stage. 
For example, data-driven methods have been used to search/design magneto-caloric materials~\cite{Bocarsly2017,Castro2020,court2021inverse}, photo-catalysts for hydrogen splitting~\cite{xiong2021optimizing}, thermoeletrics~\cite{iwasaki2019machine}. 
Furthermore, the advance of natural language processing methods for data extraction of scientific literature, also plays a key role in supporting this new paradigm. 
For example, \cite{court2018auto} developed a semi-automatic system to build a dataset of magnetic materials with their NÃ©el and Curie temperatures.  
Other work from the same authors attempted to build a model for predicting properties, e.g. magnetic and superconducting critical temperature~\cite{court_magnetic_2020}. In the specific case of superconductivity, most of data-driven works relies on a single database: SuperCon\cite{stanev_machine_2017, le2020critical,Hamlin2019SuperconductivityNR}.

In more details, SuperCon (\url{http://supercon.nims.go.jp}) is a structured database of superconductors materials and their properties, developed at the National Institute for Materials Science (NIMS) in Japan. 
At the time of writing this paper, SuperCon contains about 33000 inorganic and 600 organic materials and is the "de-facto" standard in data-driven research for superconductors materials: about 4400 articles contains the mention "\textit{supercon database}" in Google scholar. 
% One of the main problem with predicting T\textsuperscript{c} is to make reliable models which consider also non-superconducting materials or failed experiment to provide the model with the largest amount of information. Unfortunately, this is not often the case because failures and negative results are usually not reported in scientific papers.
% % I will handle this (PC) up ffrom there to line 41.

% According to~\cite{chen2020acritical}, however, predicting superconducting critical temperature T\textsubscript{c} remains a major challenge due to lack of universal theory for superconductivity and their physical models. 
% Furthermore, the absence of  structural information is "\textit{a critical gap that needs to be addressed for the development of reliable ML models}".

% Negative results are not reported - so it's very hard to create a reliable model 

% 4. Problems of supercon
% What are the current limits of SuperCon 1 from external users: data is fully manually inserted, no structure, the database contains more then 100 fields that were introduced freely 
% I want to say that the fact that supercon was built manually provided access to data in each part of the paper without limitation. While an automatic system would require different approeaches for plot/figures, tables and text. 
SuperCon is manually curated with an average update rate of 6-10 articles (corresponding to 25-30 records) every 3 months\footnote{Obtained from \url{https://dice.nims.go.jp} news feed of "2021.12.08" and "2021.09.06".}. 
The slow process makes difficult to incorporate the information from new publications in a timely manner.
%In addition, such process does not guarantee 100 percent correctness: invalid formulas, typing mistakes are known by people that have worked with the data. Unfortunately the lack of feedback system did not help to improve the quality collectively. 
%For example [PC can give me some examples]

From architecture prospective SuperCon employ the same design from more than 20 years ago. The schema has not changed since then and was designed to be flexible with a key-value approach where additional properties could be introduced and collected by curators.
% Since the research focus can shift back and forth on different aspects and properties of materials (e.g. results under applied pressure, measurement of critical current, study of specific phenomena) the underlying schema could accommodate such changes. 
Today, SuperCon contains as much as 175 properties but it lacks important information like the "applied pressure" (pressure applied to obtain superconductivity), which is relevant for researchers because it can change radically the nature of a material. 
% Unfortunately, SuperCon does not contain such property systematically. 
SuperCon also lacks the "measurement method" (the method used to measure the T\textsubscript{c}) for a) characterise multiple T\textsubscript{c} obtained from the same material and, b) allow to distinguish calculated and experimental superconductors critical temperatures. 

% 5. What are the needs? Need for a tool of methods allowing automatic extraction (both approach: general for materials or SuperCon)
In summary, there is a strong need for a tool to produce a large structured database of superconductors materials in a sustainable way. 
Ideally, our goal is to have a single tool that can be customised with ease to work in several domains. Nevertheless, at this stage, we focus only on superconductors materials research. 

% 6. in this work we present X, where we did Y and obtain Z
In this work, we present \textit{grobid-superconductors}: a system for automatically build a structured database of superconductors materials and properties from scientific literature. 
The tool is a module based on grobid~\cite{GROBID}, a machine learning library to parse and structure scientific documents. Grobid provides native support for PDFs documents, a diverse set of ML architectures (Conditional Random Field (CRF), Recurrent Neural Networks (RNN) based on bidirectional LSTM, and transformers such as BERT\cite{devlin2018bert} or SciBERT~\cite{Beltagy2019SciBERT}. 
We developed \textit{grobid-superconductors} and we trained it on SuperMat~\cite{foppiano2021supermat}, a corpus of annotated linked data of full-text articles from superconductor research.

% 7. what is our purpose? 
We established a process for creating structured databases on large quantity of documents starting from superconductors materials and properties. 
We create a database of 40324 records by processing 37770 superconductors research papers from ArXiv\footnote{\url{https://arxiv.org}} belonging to the category \textit{cond-mat.supr-cond}. We named this new database SuperCon\textsubscript{2} as the evolution of SuperCon, containing 2052 pair material-Tc including applied pressure and 3602 records containing an explicit measurement method for the measured T\textsubscript{c}.
Furthermore, we exploit the native information from PDF document provided by Grobid to construct a workflow interface for rapid database curation. 


\section{Grobid-superconductors}

% Overview from the other paper, what are the differences and some repetition on the previous paper 
\textit{Grobid-superconductor} is the extraction system that allow the process of text or PDF documents to extract materials and corresponding properties. 
We develop \textit{grobid-superconductors} as a multi-steps tool based on the \textit{Grobid} library~\cite{GROBID} following some principles previously discussed in a preliminary study~\cite{foppiano:hal-02870896}.  
The choice to develop it as a \textit{Grobid} module bring several advantages: a) It integrates with \textit{pdfalto}\footnote{\url{https://github.com/kermitt2/pdfalto}}, a specialised tool for PDF parsing which mitigate extraction issues such as the resolution of embedded fonts, invalid character encoding, and the reconstruction of the correct reading order. 
Secondly, b) it allows to access to low level layout PDF information for both machine learning and document decoration and, finally, c) it provides access to a set of high-quality, pre-trained machine learning models for structuring documents.

% Abstract vs fulltext
The scope of \textit{grobid-superconductors} is to process the full-text of scientific articles. Figures and tables body, requiring a specific process, are ignored.
At the time of writing this paper, we are aware of several related work applying machine learning on material data science that are developed with limitation of  only abstract text.
The main reason is that abstracts are usually freely available, are easier to obtain~\cite{kononova_text-mined_2019}, and contain more condensed information~\cite{yamaguchi-etal-2020-sc, court_magnetic_2020}. 
However, although working with full-text poses more challenges, it gives access to a wider spectrum of information such as background information (e.g. behaviour with under or over-doping) related to the studied materials. Even more important full-text might contains negative results (e.g. absence of superconductivity for certain samples) which are rarely discussed in the abstract. 
Such examples are needed to supply knowledge of non-superconductivity when building a superconductors prediction model~\cite{stanev_machine_2017}. 

\textit{Grobid-superconductors} provides a web API we use to integrat the extraction in an ingestion workflow (Section~\ref{sec:ingestion-workflow}) that supply PDF documents and store the resulting information in a database.

% what makes us different from previous work 
% Grobid-superconductors supports PDFs documents natively. This means that a) the system as access to additional layout information such as superscript, subscript, bold, italic, font name, font size which can be exploited for improved accuracy. And, b) we collect the coordinates in the document of any information that is extracted from PDF document.
% This system was designed by improving a previous architecture discussed in ~\cite{foppiano2019proposal} where two main steps were designed for entity extraction and linking, respectively. 
% In addition this tool supports different ML architectures, including CRF, Recurrent Neural Networks (RNN) based on LSTM, and transformers where the contextual embedding can be selected between BERT and SciBERT. 
% We released it with an open source licence and the source code is publicly available on Github. In this way the tool development could follow the community needs as well as benefit from external contributors.  

% Contrary to all the related work mentioned before, we work with fulltext instead of abstract, which give use access to a) additional information and materials that might not be mentioned in the abstract, and b) mention to cases where the experiments recorded absence of superconductivity. 

% Our obtained database differs from SuperCon because it contains several additional properties, such as applied pressure, measurement method, crystal structure, and space groups. 
% The database also provide access to the "enhanced PDFs" where all the entities are highlighted with different colours for each type on top of the original PDF, exploiting the coordinates discussed before. 
% We believe, this features, can improve both the quality of the curations and their effectiveness. In annotation tasks (e.g. NER) it was demonstrated that visual pre-annotations were improving the task over several aspects, including time consumption and error rate~\cite{Fort2010InfluenceOP, Nvol2011SemiautomaticSA, Lingren2014EvaluatingTI}.

\subsection{Architecture}

\textit{Grobid-superconductors} is structured as a three-steps process:  "Document structuring and pre-processing" (a) which transforms the input document or text into the internal representation. "Extraction" (b) is a Named Entities Recognition (NER) where the text is processed to recognise and extract material and properties. Finally, such extracted entities are combined in the final "Linking" (c) step which establish relations between them. 

\begin{figure}[ht]
\includegraphics[width=\textwidth]{overview-schema}
\caption{Overview of the architecture}
\end{figure}

\subsubsection{Document structuring and pre-processing}
The Document structuring and pre-processing component processes the PDF document into a internal model structured as a list of text sentences.

% Internal process
The input document is processed using Grobid native models as illustrated in Figure~\ref{fig:grobid-document-processing}. After being transformed in XML by \textit{pdfalto}, it is passed to the "segmentation model" for recognising the macro areas (header, body and annex). As a result, the obtained sections are passed to specialised models in a cascade architecture. The header is further processed by the Header model, which further extracts the structure. We selectively extract title, abstract, keywords, publisher, publication year, doi and ignore other items, such as authors, publication date. 
Body and annexes are processed by the "Fulltext" model that recognises paragraphs, reference markers (decorations (also called \textit{reference callout}) in the text referring to the citation references at the end of the paper), tables and figure zones.
Tables and figures zones are further decomposed by the respective models and we retain only the caption text. 

\begin{figure}[ht]
\label{fig:grobid-document-processing}
\includegraphics[width=\textwidth]{grobid-extraction-schema}
\caption{Cascade architecture in the document structuring with Grobid.}
\end{figure}

The text is finally passed to a sentence segmenter (we use Apache OpenNLP (\url{https://opennlp.apache.org})) which splits paragraphs into sentences. We use the reference markers collected from the text improve the sentence segmentation process: the segmentation is cancelled if the end of sentence falls within the boundaries of one reference marker. For example is often the case that a reference in the form of \textit{Foppiano et. al.} is mistakenly recognised as the end of the sentence. This method is very effective to enhance sentence segmentation in scientific text. 
The sentences are accumulated into special objects that represent general text passages containing the following properties: 
\begin{itemize}
    \item \textit{LayoutTokens} represent the token with layout information, such as style (italic, bold, superscript, and subscript), font (font type, font size), positions (coordinates (a list of pairs x,y), index position within the text chain), 
    \item \textit{section} contains the main sections: header, body and annex as they are divided from the Grobid "document segmentation model", 
    \item \textit{subsection} contains further division within the same section: paragraph, table or figure caption, abstract, title, 
    \item \textit{spans} are allocated to carry the extracted entities in the following step. One \textit{span} represents one entity with its type and position within the text. Spans can also store additional attributes using a key-value approach. For example a T\textsubscript{c} is represented by their form in the text but we can attach additional formats, for example their normalised representation. 
\end{itemize}

During the \textit{Document structuring and pre-processing}, the process extracts also the bibliographic data of the paper by combining the extracted information with a matching service to consolidate them against CrossRef\footnote{\url{https://www.crossref.org}}. In this way we ensure that the bibliographic data can match the publisher's information even if the paper was obtained from different sources, such as ArXiv. We collect the following bibliographic: title, authors, DOI, publisher, journal, and year of publication.

The following step of Extraction and Linking are enriching the internal structure by attaching additional structured information. 

\subsubsection{Extraction}

The second step in the architecture is the \textit{Extraction}, a named entities recognition (NER) task.
We designed this component as aggregation of multiple parsers with a final post-processing. Each parser focuses on specific extractions and combining multiple machine learning models and rule-based algorithms.
The machine learning models can be trained using multiple architectures thanks to the integration between Grobid and the DeLFT (Deep Learning For Text) library~\cite{DeLFT}. 
The rule-based parsers are implemented using SpaCy (\url{https://spacy.io} entity ruler.

\begin{figure}[ht]
\label{fig:extraction-ml-models-cascade-architecture}
\includegraphics[width=\textwidth]{extraction-schema-2}
\caption{Cascade architecture combining the superconductors and material parsers.}
\end{figure}

Figure~\ref{fig:extraction-ml-models-cascade-architecture} illustrates the data flow with the interaction between parsers and the extracted entity types. 

\paragraph{Superconductors parser} combines two ML models (\textit{superconductors ML model} and \textit{quantities ML model}) and two RB extractors (\textit{crystal structures entity ruler} and \textit{space groups entity ruler}). 
The \textit{superconductors ML model} is trained trained using SuperMat~\cite{foppiano2021supermat} (Table~\ref{tab:evaluation-10fold-superconductors-parser}) and aims to extract the main entities from the superconductors domain.
We provide models trained using Conditional Random Field (CRF), Recurrent Neural Networks (RNN) using Bidirectional LSTM with CRF (BidLSTM\_CRF~\cite{Lample2016NeuralAF}), and Transformers (SciBERT~\cite{Beltagy2019SciBERT}).
The \textit{quantities ML model} is a model specialised in extracting quantities of measurements from the \textit{grobid-quantities}~\cite{foppiano2019quantities} project and trained on scientific text.
We filter entities extracted by the \textit{quantities ML model} to retain only temperatures and pressures and we merge them with the entities extracted by the \textit{superconductors ML model} to remove duplicates or keep the largest matches.
Finally, the rule-based components are extracting the crystal structures, the space groups and the unit cells. We use rule based because the number of items is finite with lower variation.  
The extracted entities are summarised in Table~\ref{tab:superconductors-parser-entities}. More details about the definition of the machine learning entities can be obtained in the work on SuperMat~\cite{foppiano2021supermat}.

\begin{table}[ht]
\centering
\scalebox{0.7}{
\begin{tabular}{m{19em} m{30em}}
\hline 
\textbf{Entity} (\textbf{Tag})& \textbf{Description} \\ 
\hline
\hline
\multicolumn{2}{c}{\textbf{Machine learning}} \\
\hline
Material (\texttt{<material>}) & Materials and samples names, formulas, including stochiometric formulas, substitution variables of values and elements, shape, doping, substrate \\
Class (\texttt{<class>}) & Groups of materials having similar characteristics or common strategic compounds that define their nature \\
T\textit{c} value (\texttt{<tcValue>})& The value of the superconductors critical temperature\\
T\textit{c} expressions (\texttt{<tc>}) & Expressions in the text that provide information about the phenomenon of superconductivity related to a value, interval or variation of the T\textsubscript{c}\\
Measurement methods (\texttt{<me\_method>}) & Techniques used to measure or calculate the presence of superconductivity. \\
Applied pressure (\texttt{<pressure>}) & Applied pressure when superconductivity is recorded\\
\hline
\hline
\multicolumn{2}{c}{\textbf{Rule-based}} \\
\hline
Crystal structure (\texttt{<crystal-structure>}) & Geometry of arrangement of particles in the unit cell\\
Space groups  (\texttt{<space-group>}) & Symmetry group, usually in three dimensions\\
Unit cells (\texttt{<unit-cells>}) & A repeating unit formed by the vectors spanning the points of a lattice\\
\hline
\end{tabular}
}
\label{tab:superconductors-parser-entities}
\caption{Synthesis of the superconductors parser entities. }
\end{table}

\paragraph{Material parser} is used to further structure the \texttt{<material>} entities from the \textit{Superconductors parser}. 
First, the material raw string is passed through a \textit{material ML model} to extract several attributes, summarised in Table~\ref{tab:material-parser-entities}. 
This model is trained using annotated material raw strings (Table~\ref{tab:evaluation-10fold-material-parser}). 
We provide models trained using CRF and RNN.
%Since the performances with CRF or RNN are above 90\%, we do not supply the model based on transformers for this task. 
Then, we apply several processing based on which information are available: 
\begin{itemize}
    \item we decompose the formula into a structured composition and we identify each group of compounds and values using a combination between mat2chem~\cite{kononova_text-mined_2019} and Pymatgen~\cite{Ong2013}, 
    \item we obtain the formula corresponding to the material name, if the name available and the formula is not (e.g. hydrogen to \textit{H}), 
    \item we calculate the classes from the formula composition, using a simple algorithm we have designed which can return values such as Cuprate, Oxides, Alloys 
    \item using the extracted variables and values we apply permutation to the stochiometric formula to resolve the substitutions. For example the stochiometric formula \texttt{La 4 Fe 2 A 1-x O 7 (A=Mg,Co; x=0.1,0.2)} will permute the values of A and x to obtain \texttt{La 4 Fe 2 Mg 0.9 O 7}, \texttt{La 4 Fe 2 Mg 0.8 O 7}, \texttt{La 4 Fe 2 Co 0.9 O 7}, \texttt{La 4 Fe 2 Co 0.8 O 7}.
\end{itemize}

\begin{table}[ht]
\centering
\scalebox{0.7}{
\begin{tabular}{m{16em} m{30em}}
\hline 
\textbf{Entity} (\textbf{Tag})& \textbf{Description} \\ 
\hline
Name (\texttt{<name>}) & The canonical name of a material (e.g. Hydrogen, PCCO, Carbon) \\
Formula (\texttt{<formula>}) & Chemical formula of the material (e.g. \texttt{Pr1.869Ce0.131CuO 4-}, \texttt{MgB2}, \texttt{La 2-x Sr x CuO 4}) \\
Doping (\texttt{<doping>})& Doping ratio and doping materials that are adjointed to the material name (e.g. \\
Shape (\texttt{<shape>}) & shape of the material (e.g. single crystal, polycrystaline, thin film, powder, film) \\
Substitution variables (\texttt{<variable>}) & Variables that can be substituted in the formula. \\
Substitution values (\texttt{<value>}) & Values expressed in the stoichiometric doping. \\
Substrate (\texttt{<substrate>}) & Substrates as defined in the material name \\
Fabrication (\texttt{<fabrication>}) & Represent eventual additional information that are not belonging to any of the previous tags  (e.g. intercalated, electron-doped)\\
\hline
\end{tabular}
}
\label{tab:material-parser-entities}
\caption{Synthesis of the material parser entities. }
\end{table}

\paragraph{Post-processing} The post-processing is the final step of the extraction and has the objective of running aggregations at document level. We utilise the decomposed formulas to aggregate the mention in the document that is referring to the same material. 
For example we could aggregate simple cases such as \texttt{hydrogen} and \texttt{H} but also a complex stochiometric formula (e.g. \texttt{La 2 Fe 1-x O 7 (x = 0.1, 0.2)} with formulas discussed further in the paper. 

\paragraph{Discussion on machine learning evaluation}

In this section we discuss the machine learning evaluation scores for the superconductors and materials ML models. The evaluation was performed using the 10-fold cross validation to obtain precision, recall and f1 score. The overall scores were calculated using the micro average which compensate the different balance of entities annotations. 

The superconductors model is detailed in Table~\ref{tab:evaluation-10fold-superconductors-parser} and obtained the best result using transformers, in particular SciBERT had a F1 score of 83.46\% and recall around 85\%. Both deep learning approaches (SciBERT and RNN) showed most of the gains in the recall.
We noticed that the CRF approach had more difficulty to extract large entities which were partially extracted. 
In term of performances, the CRF is the fastest approach and the SciBERT is the slowest, limited by the large neural network that needs to be loaded in memory.
Overall, the BidLSTM+CRF (RNN) had the best trade-off between performances and throughput, as compared with the SciBERT approach. 

We notice that \texttt{<pressure>} results in lower performances and the reason due to the lack of enough training data as compared with the rest of the labels. 

\begin{table}[ht]
\centering
\scalebox{0.7}{
\begin{tabular}{l|ccc|ccc|ccc}
& \multicolumn{3}{c}{\textbf{CRF}} & \multicolumn{3}{c}{\textbf{BidLSTM+CRF} (RNN)} & \multicolumn{3}{c}{\textbf{SciBERT} (transformers)}\\ 
\textbf{Label} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\ \hline
\texttt{<class>}         & 79.69 & 75.54 & 77.55 & 81.84 & 83.96 & 82.85 & 79.58 & 85.79 & 82.56\\
\texttt{<material>}      & 82.9  & 81.33 & 82.1  & 85.18 & 83.86 & 84.51 & 83.89 & 86.13 & 84.99\\
\texttt{<me\_method>}    & 82.47 & 81.26 & 81.84 & 83.51 & 83.37 & 83.43 & 83.92 & 86.50 & 85.19\\
\texttt{<pressure>}      & 65.03 & 54.01 & 58.26 & 63.79 & 73.24 & 67.98 & 63.92 & 71.18 & 67.27\\
\texttt{<tc>}            & 84.63 & 80.73 & 82.63 & 83.70 & 81.66 & 82.66 & 80.91 & 83.00 & 81.94\\
\texttt{<tcValue>}       & 79.3  & 74.95 & 76.97 & 73.23 & 80.73 & 76.76 & 76.74 & 85.00 & 80.65\\
\hline
All (micro avg) & 82.43 & 79.68 & 81.03 & \textbf{83.01} & 82.89 & 82.95 & 83.01 & \textbf{85.06} & \textbf{83.46}\\
\hline
\end{tabular}
}
\caption{Evaluation scores for the superconductor models using 10-fold cross-validation. }
\label{tab:evaluation-10fold-superconductors-parser}
\end{table}

The material parser is provided in two architectures: CRF and BidLSTM+CRF because the performances are above 95\% and the SciBERT approach is very slow. 
Surprisingly, this model performs better using the CRF than the RNN. We notice that \texttt{<fabrication>} and \texttt{<substrate>} have relatively low results, for the same reason of \texttt{<pressure>} of lack of enough training data.
[PO we can potentially extend this part adding an additional section discussing NER]

\begin{table}[ht]
\centering
\begin{tabular}{l|ccc|ccc}
& \multicolumn{3}{c}{\textbf{CRF}} & \multicolumn{3}{c}{\textbf{BidLSTM+CRF} (RNN)}\\ 
\hline \textbf{Label} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\ \hline

\texttt{<doping>}      & 86.31   & 86.2     & 86.22 & 84.87 &  87.82 & 86.29   \\
\texttt{<fabrication>} & 60.93   & 50.3     & 54.27 & 7.43  &  13.33 & 8.94   \\
\texttt{<formula>}     & 98.06   & 97.93    & 98    & 97.33 &  97.51 & 97.42  \\
\texttt{<name>}        & 97.25   & 96.95    & 97.09 & 96.08 &  96.70 & 96.38  \\
\texttt{<shape>}       & 97.26   & 97.15    & 97.2  & 96.38 &  94.93 & 95.64  \\
\texttt{<substrate>}   & 83.33   & 60.83    & 68    & 65.76 &  97.50 & 77.66  \\
\texttt{<value>}       & 95.99   & 95.99    & 95.99 & 95.28 &  95.84 & 95.56  \\
\texttt{<variable>}    & 99.24   & 98.85    & 99.04 & 98.94 &  98.99 & 98.96  \\
\hline
all  (micro avg)       & 96.87   & 96.51    & 96.68  & 95.75 &   96.44  &  96.09  \\
\hline
\end{tabular}
\caption{Evaluation scores of 10-fold cross-validation of the material parser. }
\label{tab:evaluation-10fold-material-parser}
\end{table}


\subsubsection{Linking}

%Introduction of the linking
The Linking is the final step of the processing that aims to link together entities previously extracted.
%Objective of the linking
We can formalised it as follows. \textit{Given a text T and two or more entities e\textsubscript{1}...e\textsubscript{n} of two types t\textsubscript{1} and t\textsubscript{2}, determine links between entities of type t\textsubscript{1} can be linked to entities of type t\textsubscript{2} .} 

%We have experimented several options: dependency parsing, rule-based, and sequence labelling. 
Our implementation uses a main rule-based method and a secondary approach based on sequence labelling. The dependency parsing was studied using several approaches from scientific literature~\cite{yoshikawa:2017acl, Tiktinsky2020pyBARTES, swayamdipta:17, zhou-zhao-2019-head} and it was discarded because a) it is challenging to find parser performing well on scientific text, and, by consequence, b) our implementation was converging toward a complex set of rules to compensate for the poor performances of the dependency parser.

% Algorithm in brief (three or four different scenario): 
\paragraph{T\textsubscript{c} classification}
For linking the T\textsubscript{c} to any other entity type, there is an classification engine for deciding whether the temperature entity is  "a superconducting critical temperature" or not. 
The classification is rule-based and combines the extracted entities of T\textsubscript{c} expressions (label \texttt{<tc>}) with a set of predefined standard names. 
If a T\textsubscript{c} is not considered a "superconducting critical temperature" is excluded from the list of possible linking candidates. 

\paragraph{Rule-based linking}
The rule-based approach works with two main scenarios: a) if entities to be linked have cardinality one in the sentence, they are linked automatically, and b) when their cardinality is higher then they are linked "by distance" or "in order", depending on the structure of the sentence. 
If the word \textit{"respectively"} appears in the sentence, entities are linked "in order", otherwise they are linked "by distance". 
For example, the sentence:  
\begin{displayquote}
P-or Ba-122  and Co-doped Ba-122 have lower T c s of about 30 K and 24 K, respectively, which makes helium free operation questionable.
\end{displayquote}
containing the word "respectively" and therefore is processed by linking the materials in order: \textit{P-or Ba122} is assigned to \textit{30 K} and \textit{Co-doped Ba-122} to \textit{24 K}.
The "in order" method is very sensible to missing entities, for example if one entity is not extracted by mistake it will rick to incorrectly assign all the entities in the block. To avoid this, if the entities are unbalanced we reduce the search space as follows: 
\begin{itemize}
    \item m entities of type\textsubscript{1} vs n entities of type\textsubscript{2} with $m > n$: we shift the starting point to start at the second entity 
    \item m entities of type\textsubscript{1} vs n entities of type\textsubscript{2} with $m < n$: we shift the ending point at the n\textsubscript{-1} entity
\end{itemize}
With this sentence structure we know that the entities are in order, however if one is missing we just ignore the entities in surplus without disrupting the assignment. 

We define the distance measurement \textit{d} as a value calculated in numbers of characters from the centre of the entities. 
If the entity is surrounded by parenthesis we expand it to the whole parenthesis and centre is shifted and all its content refers to the same entity. 
As an example, in the sentence
\begin{displayquote}
We tested two materials MgB2 (Tc = 39K) and LaFe (Tc = 16K).
\end{displayquote}
both temperatures are expanded to their containing parenthesis e.g. \texttt{39K} to \texttt{(Tc = 39K)}, moving the centre toward the left. 
As a result, the distance tend to be shorter to entities immediately closed to the whole parenthesis block and the impact of the dimension of the whole parenthesis block is reduced when calculating the distance (the entity could be on each side within the parenthesis). 

The distance calculation is also adjusted with the addition of "penalties", that is, the distance is doubled when certain keywords such as "and", ", ", "." representing logical separation of predicates appear in the sentence. The rationale is that element between two separated predicates are likely not to be linked although they might be at a smaller distance. 

\paragraph{Sequence labelling linking} We designed the linking based on sequence labelling as a simple task where each token has been assigned two possible labels: \texttt{<link\_left>} or \texttt{<link\_right>}. The post-processing decodes the labels and translated them into linking between entities. 
This approach has a obvious limitation to linking subsequent entities.  

\paragraph{Results} In our implementation we focus on three types of relationships: 
\begin{itemize}
    \item \textbf{material-tcValue} links a material with its corresponding superconducting critical temperature value, 
    \item \textbf{tcValue-pressure} connects a superconducting critical temperature value with its related critical pressure, and 
    \item \textbf{me\_method-tcValue} connects the superconducting critical temperature value to its corresponding measurement method.
    % \item \textbf{material-crystal\_structure} link the material with their crystal structure, and 
    % \item \textbf{material-space\_group} to link the material to their space groups.
\end{itemize}

\paragraph{Discussion on evaluation}
The evaluation is shown in Table \ref{table:evaluation-linking}. 
The rule-based was tested using the SuperMat~\cite{foppiano2021supermat} corpus which contains linked entities. Each test was framed to test the linking between two entities type. 
The result of the \texttt{material-tcValue} obtained above 80\% F1 score with a precision of 88\%. We would like to stress we strive to obtain higher precision scores. 

The approach with sequence labelling was less performant, with scores of 69.16\%, 69.76\%, and 44.65\% F1 score, respectively. In general, the effectiveness of this methodology is dubious, however the amount of training data that can be used is smaller than for NER. 
This is the main cause of the lower score of \texttt{tcValue-me\_method}. 


\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\hline \textbf{Linked entities} (Method) & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\ \hline
\textbf{material-tcValue} (Rule-based)  & 88.00 	&   74.00      &	81.00      \\
\textbf{material-tcValue} (CRF)         & 68.52 &	70.11   &  69.16    \\
\textbf{tcValue-pressure} (CRF)         & 72.92 &	67.67   &  69.76    \\
\textbf{tcValue-me\_method} (CRF)       & 49.99 &	45.21   &  44.65   \\
\hline
\end{tabular}
\label{table:evaluation-linking}
\caption{Evaluation scores of the linking methods. }
\end{table}

\subsection{End to end evaluation}

% What is the end 2 end evaluation? 
The end to end evaluation (e2e evaluation) measures the real capability from beginning to the end of the process. The evaluation of the different steps are useful but not sufficient to assess their quality. 

We perform e2e evaluation manually. First we process a dataset of 500 documents, then we select a representative amount. In Table~\ref{table:end2end-evaluation} we illustrate the results. 

The F1-score is close to 70\% with precision at 73\% and recall at 66\%.

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\hline \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} & \textbf{Support} \\ \hline
73.86  &	66.33 &	69.90 & 597\\
\hline
\end{tabular}
\label{table:end2end-evaluation}
\caption{End 2 end evaluation scores. }
\end{table}

Discussion of the results, which problems are currently emerging what are the prospective for the future?

\section{Supercon\textsuperscript{2}}

Introduction of the database, which data was extracted and the format 

\subsection{Ingestion workflow}
\label{subsec:ingestion-workflow}

\begin{figure}[ht]
\includegraphics[width=\textwidth]{workflow-schema-1}
\caption{Schema of the ingestion workflow}
\end{figure}


Quickly discussion on the ingestion workflow 


\subsection{User interface}
\label{sucsec:supercon2-user-interface}


\section{Conclusion}
\label{sec:conclusion}

\bibliography{bibliography}
\bibliographystyle{plain}


\end{document}
